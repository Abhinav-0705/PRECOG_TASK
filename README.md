# PRECOG_TASK

# Task0 — Submission Repo (Tasks 0–4)

This folder contains the full code + **logged, reproducible results** for Task0.

**Mandatory results notebook (read this first):**
- `notebooks/Tasks_results.ipynb` — runs the full pipeline and logs outputs/plots/results.

If you only have time to check one artifact, check the notebook above.

---

## What’s included (directory structure)

- `notebooks/`
	- `Tasks_results.ipynb` *(mandatory)*: experiments + results + personal test.
- `scripts/`
	- `clean_text.py` — clean Gutenberg texts and build paragraph dataset
	- `extract_topics.py` — TF‑IDF + NMF topic keywords
	- `prepare_gemini_prompts.py` — build JSONL prompts for Class2/Class3 generation
	- `tierA_trainer.py`, `tierABC_trainer.py`, `tierB_trainer.py`, `tierC_trainer.py` — model training + evaluation
	- `task1_*` — fingerprint feature extraction + bootstrap tests
- `data/`
	- `analysis/` — saved metrics/models/results (CSV/JSON/PNG + joblib bundles)
	- (optional) `cleaned/`, `topics/`, `prompts/` — generated by scripts when you run the pipeline
- `REPORT.md` — written report (methodology + results)
- `requirements.txt` — main dependencies
- `requirements_tierBC.txt` — optional pinned deps for Tier B/C environment

**Not intended for GitHub commit:** local caches (`.cache`, `.hf_cache`) and virtualenv (`.venv`).

---

## Setup

Create a virtual environment and install dependencies:

```bash
python3 -m venv .venv
source .venv/bin/activate
python3 -m pip install -r requirements.txt
python3 -m spacy download en_core_web_sm
```

---

## How to reproduce (end-to-end)

### 1) Build dataset artifacts

```bash
python3 scripts/clean_text.py --data-dir data --out-dir data/cleaned
python3 scripts/extract_topics.py --clean-dir data/cleaned --out-dir data/topics --n-topics 7
python3 scripts/prepare_gemini_prompts.py --topics-file data/topics/topics_n7.json --paragraphs data/paragraphs.csv --out-dir data/prompts --n-samples 500
```

### 2) Train/evaluate detectors

```bash
python3 scripts/tierA_trainer.py --class0 Class0 --class3 Class3 --out data/analysis
python3 scripts/tierABC_trainer.py --class0 Class0 --class2 Class2 --class3 Class3 --out data/analysis
python3 scripts/tierB_trainer.py --out data/analysis
python3 scripts/tierC_trainer.py --out data/analysis
```

### 3) Open the notebook (results are logged there)

Open `notebooks/Tasks_results.ipynb` and run all cells.

---

## Notes on “what I did / didn’t do”

- Live Gemini API calls are intentionally not executed here (requires secrets). Prompt JSONL is prepared.
- The submission emphasizes transparency and reproducibility via the notebook + saved `data/analysis/` artifacts.